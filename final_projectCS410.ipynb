{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f74ad3a9-f8e2-423c-8d7d-7ec7bdd8b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c60b9f38-4b00-482b-8576-91115826f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from treeinterpreter import treeinterpreter as ti\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d1219a1-459b-49c1-8274-3c0fb1b5665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/yuchen/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/yuchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/yuchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "sentiment_i_a = SentimentIntensityAnalyzer()\n",
    "\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf477297-7a1d-4e47-9229-ef830ab6ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+', '[URL]', tweet)  # Replace URLs with [URL]\n",
    "    tweet = re.sub(r'#', '', tweet)  # Remove '#' from hashtags\n",
    "    tweet = re.sub(r'([!?.])\\1+', r'\\1', tweet)  # Normalize repeated punctuation\n",
    "    return tweet\n",
    "\n",
    "# Load dataset\n",
    "splits = {'train': 'sent_train.csv', 'validation': 'sent_valid.csv'}\n",
    "train_df = pd.read_csv(\"hf://datasets/zeroshot/twitter-financial-news-sentiment/\" + splits[\"train\"])\n",
    "validation_df = pd.read_csv(\"hf://datasets/zeroshot/twitter-financial-news-sentiment/\" + splits[\"validation\"])\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['processed_text'] = train_df['text'].apply(preprocess_tweet)\n",
    "validation_df['processed_text'] = validation_df['text'].apply(preprocess_tweet)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "\n",
    "# Tokenize the preprocessed text\n",
    "train_encodings = tokenizer(list(train_df['processed_text']), padding=True, truncation=True, max_length=128)\n",
    "validation_encodings = tokenizer(list(validation_df['processed_text']), padding=True, truncation=True, max_length=128)\n",
    "train_encodings['labels'] = train_df['label']\n",
    "validation_encodings['labels'] = validation_df['label']\n",
    "# Ready for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b53e2ce6-b272-4b5a-9fe2-2cec45930a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(train_encodings)\n",
    "validation_dataset = Dataset.from_dict(validation_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18784899-7a45-42ba-be74-c66ded00a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1194' max='1194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1194/1194 21:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.618100</td>\n",
       "      <td>0.474916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.404700</td>\n",
       "      <td>0.453944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1194, training_loss=0.4830047377389879, metrics={'train_runtime': 1313.7378, 'train_samples_per_second': 14.528, 'train_steps_per_second': 0.909, 'total_flos': 971008762470516.0, 'train_loss': 0.4830047377389879, 'epoch': 2.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\", num_labels=3)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b5b344f-4474-4f11-af54-e910bf382f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('/Users/yuchen/Downloads/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2b5adb42-98b4-4048-ba2f-9ae66e516462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Datetime, Tweet Id, Text, Username, Cleaned_Text, date]\n",
      "Index: []\n",
      "Oldest Date: 2022-11-21\n",
      "Most Recent Date: 2023-02-06\n"
     ]
    }
   ],
   "source": [
    "df_nvda = pd.read_csv('/Users/yuchen/Downloads/Nvidia-Tweets.csv')\n",
    "df_nvda = df_nvda.dropna(subset=['Text'])  # Drop rows with missing 'Text' or 'Tweet Id'\n",
    "\n",
    "import contractions\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+', '[URL]', tweet)  # Replace URLs with [URL]\n",
    "    tweet = re.sub(r'#', '', tweet)  # Remove '#' from hashtags\n",
    "    tweet = re.sub(r'([!?.])\\1+', r'\\1', tweet)  # Normalize repeated punctuation\n",
    "    tweet = tweet.lower()  # Convert to lowercase\n",
    "    tweet = re.sub(r'[^a-zA-Z\\s]', '', tweet)  # Remove non-alphabetic characters\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet) #remove emojis\n",
    "    tweet = contractions.fix(tweet)  # Expand contractions\n",
    "    return tweet\n",
    "\n",
    "\n",
    "df_nvda['Cleaned_Text'] = df_nvda['Text'].astype(str).apply(preprocess_tweet)\n",
    "\n",
    "\n",
    "# Attempt to convert 'Datetime' column to datetime, invalid entries will become NaT (Not a Time)\n",
    "df_nvda['Datetime'] = pd.to_datetime(df_nvda['Datetime'], errors='coerce')\n",
    "\n",
    "# Drop rows where 'Datetime' could not be converted (NaT values)\n",
    "df_nvda = df_nvda.dropna(subset=['Datetime'])\n",
    "\n",
    "# Proceed with extracting the date\n",
    "df_nvda['date'] = df_nvda['Datetime'].dt.date\n",
    "\n",
    "# Check rows where Datetime is invalid (NaT)\n",
    "invalid_rows = df_nvda[df_nvda['Datetime'].isna()]\n",
    "print(invalid_rows)\n",
    "\n",
    "\n",
    "# Get the oldest (earliest) date\n",
    "oldest_date = df_nvda['date'].min()\n",
    "\n",
    "# Get the most recent date\n",
    "most_recent_date = df_nvda['date'].max()\n",
    "\n",
    "print(f\"Oldest Date: {oldest_date}\")\n",
    "print(f\"Most Recent Date: {most_recent_date}\")\n",
    "\n",
    "df_nvda = df_nvda[~df_nvda['Datetime'].dt.weekday.isin([5, 6])]\n",
    "\n",
    "# If you have a list of holidays, e.g., holidays in the format of datetime objects\n",
    "holidays = pd.to_datetime(['2024-12-25', '2024-01-01'])  # Example of holidays\n",
    "df_nvda = df_nvda[~df_nvda['date'].isin(holidays)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5080fa1-0622-43c2-863d-ea65ee5e05f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1925: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Specify the path to your saved model and tokenizer files\n",
    "model_path = \"/Users/yuchen/Downloads/\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path+'config.json')\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# If using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prepare your dataset\n",
    "input_data = df_nvda['Cleaned_Text'].tolist()\n",
    "# Tokenize the entire dataset\n",
    "inputs = tokenizer(input_data, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Convert the tokenized data into a TensorDataset\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "84a71dec-25ad-45ca-9501-a89fb39346b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoader to create batches\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "426b2c55-6d3f-4e7c-a5a6-c2bbbc292498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 650/650 [3:00:48<00:00, 16.69s/batch]\n"
     ]
    }
   ],
   "source": [
    "# List to store predictions for each batch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List to store predictions for each batch\n",
    "predictions = []\n",
    "\n",
    "# Process each batch with tqdm to track progress\n",
    "for batch in tqdm(dataloader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "    input_ids, attention_mask = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Extract logits\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities using softmax (for classification tasks)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get predicted class (for classification tasks)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1)\n",
    "    \n",
    "    # Append predictions to the list\n",
    "    predictions.extend(predicted_class.cpu().numpy())\n",
    "\n",
    "# Save predictions to a DataFrame (or list of predicted classes)\n",
    "df_predictions = pd.DataFrame(predictions, columns=['Predicted_Class'])\n",
    "\n",
    "# Optionally, you can add the predictions to the original DataFrame (df_nvda)\n",
    "df_nvda['Predicted_Class'] = df_predictions['Predicted_Class']\n",
    "\n",
    "# Save to CSV or other file formats\n",
    "df_nvda.to_csv('nvda_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4d1c3-3999-441d-b308-e67da5a9c928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20674a7a-1b26-433f-b600-77bbb0ff91b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "24ba9c4e-6ba0-4340-ab61-36ce50bc6f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP/lib/python3.9/site-packages/yfinance/utils.py:771: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.25      0.25         4\n",
      "           1       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.38      0.38      0.38        10\n",
      "weighted avg       0.40      0.40      0.40        10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.43      0.50      0.46         6\n",
      "\n",
      "    accuracy                           0.30        10\n",
      "   macro avg       0.21      0.25      0.23        10\n",
      "weighted avg       0.26      0.30      0.28        10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.25      0.25         4\n",
      "           1       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.38      0.38      0.38        10\n",
      "weighted avg       0.40      0.40      0.40        10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.25      0.25         4\n",
      "           1       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.38      0.38      0.38        10\n",
      "weighted avg       0.40      0.40      0.40        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the stock symbol and date range\n",
    "symbol = 'NVDA'\n",
    "start_date = '2022-11-21'\n",
    "end_date = '2023-02-06'\n",
    "\n",
    "# Get the stock data from Yahoo Finance\n",
    "stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Select the 'Adj Close' column\n",
    "adj_close = stock_data['Adj Close']\n",
    "\n",
    "# Drop non-trading days (if any)\n",
    "adj_close = adj_close.dropna()\n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity  # returns a score between -1 and 1\n",
    "\n",
    "df_nvda['Sentiment'] = df_nvda['Cleaned_Text'].apply(get_sentiment)\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply sentiment analysis to the 'Cleaned_Text' column\n",
    "df_nvda['Sentiment_Score'] = df_nvda['Cleaned_Text'].apply(lambda tweet: sia.polarity_scores(tweet)['compound'])\n",
    "df_nvda['Predicted'] = df_nvda['Predicted_Class'].apply(lambda score: 1 if score == 1 else (-1 if score == 0 else 0))  \n",
    "daily_data = df_nvda.groupby('date').agg({\n",
    "    'Predicted':'sum',\n",
    "    'Sentiment': 'sum',\n",
    "    'Sentiment_Score':'sum',\n",
    "    'Tweet Id': 'count'  # Count the number of tweets\n",
    "}).reset_index()\n",
    "# Classify sentiment\n",
    "\n",
    "daily_data['SentimentScore_BLOB'] = daily_data['Sentiment']#.apply(lambda score: 1 if score > 0 else (-1 if score < 0 else 0))  \n",
    "daily_data['SentimentScore_VADER'] = daily_data['Sentiment_Score']#.apply(lambda score: 1 if score > 0 else (-1 if score < 0 else 0))  \n",
    "daily_data['SentimentScore_BERT'] = daily_data['Predicted']\n",
    "daily_data.date = pd.to_datetime(daily_data.date)\n",
    "adj_close.index = pd.to_datetime(adj_close.index) \n",
    "merged_df = pd.merge(daily_data, adj_close, left_on='date', right_index=True, how='left')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "merged_df['price move'] = merged_df['Adj Close'].diff().shift(-1)\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "X = merged_df[['Adj Close']]\n",
    "y = merged_df['price move'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "X = merged_df[['SentimentScore_BLOB','Adj Close']]\n",
    "y = merged_df['price move'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "X = merged_df[['SentimentScore_VADER','Adj Close']]\n",
    "y = merged_df['price move'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "X = merged_df[['SentimentScore_BERT','Adj Close']]\n",
    "y = merged_df['price move'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f7bd5ea7-73df-4a77-b048-41aa58293908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Profit: $384.48\n",
      "Average Daily Return: 0.85%\n",
      "Win Rate: 56.52%\n",
      "Maximum Drawdown: $244.86\n"
     ]
    }
   ],
   "source": [
    "merged_df['daily_return'] = merged_df['Adj Close'].pct_change()\n",
    "\n",
    "# Assume starting capital\n",
    "initial_capital = 1000\n",
    "merged_df['daily_profit'] = initial_capital * merged_df['daily_return']  # Profit for fixed capital\n",
    "merged_df['cumulative_profit'] = merged_df['daily_profit'].cumsum()\n",
    "\n",
    "# Metrics\n",
    "total_profit = merged_df['daily_profit'].sum()\n",
    "average_daily_return = merged_df['daily_return'].mean()\n",
    "win_rate = (merged_df['daily_profit'] > 0).mean() * 100\n",
    "max_drawdown = (merged_df['cumulative_profit'].cummax() - merged_df['cumulative_profit']).max()\n",
    "\n",
    "# Print results\n",
    "print(f\"Total Profit: ${total_profit:.2f}\")\n",
    "print(f\"Average Daily Return: {average_daily_return:.2%}\")\n",
    "print(f\"Win Rate: {win_rate:.2f}%\")\n",
    "print(f\"Maximum Drawdown: ${max_drawdown:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "192e6048-3aca-4698-9ba1-a6ad8c455834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare your dataset\n",
    "input_data = df_nvda['Cleaned_Text'].tolist()\n",
    "# Tokenize the entire dataset\n",
    "inputs = tokenizer(input_data, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Convert the tokenized data into a TensorDataset\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "# Define DataLoader to create batches\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# List to store predictions for each batch\n",
    "predictions = []\n",
    "\n",
    "# Process each batch\n",
    "for batch in dataloader:\n",
    "    input_ids, attention_mask = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Extract logits\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities using softmax (for classification tasks)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get predicted class (for classification tasks)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1)\n",
    "    \n",
    "    # Append predictions to the list\n",
    "    predictions.extend(predicted_class.cpu().numpy())\n",
    "\n",
    "# Save predictions to a DataFrame (or list of predicted classes)\n",
    "df_predictions = pd.DataFrame(predictions, columns=['Predicted_Class'])\n",
    "\n",
    "# Optionally, you can add the predictions to the original DataFrame (df_nvda)\n",
    "df_nvda['Predicted_Class'] = df_predictions['Predicted_Class']\n",
    "\n",
    "# Save to CSV or other file formats\n",
    "df_nvda.to_csv('nvda_predictions.csv', index=False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9c31581a-2add-45f0-8d5b-8f6175120c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nvda['Predicted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bce2fd-7b0d-4992-b61e-769bfd462431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
